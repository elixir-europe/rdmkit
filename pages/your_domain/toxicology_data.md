---
title: Toxicology data
search_exclude: true
contributors: [Manuel Pastor, Janet Piñero Gonzalez, Juan Manuel Ramírez-Anguita, Ferran Sanz, Miguel Angel Mayer]
page_id: toxicology data
related_pages: 
  - your_tasks: [data analysis]
---

## Introduction

Toxicology is focused on the study of the adverse effects that occur in living organisms due to their interaction with chemicals. These chemicals range from substances found in nature to those made in the laboratory for many purposes (drugs, agrochemicals, pesticides, dyes, food additives, cosmetics, household products, etc.). A part of the toxicological research is devoted to the study of the adverse effects generated by chemicals in humans, while another part is devoted to the study of the noxious effects of the chemicals in the environment. The adversity is observed for a compound at a certain concentration. Consequently, hazard characterization should always consider exposure data.
Toxicology was traditionally an observational science that obtained an important part of its data by means of experiments carried out on animals. However, the limitations of animal models to produce human-relevant data, as well as the implementation of the 3R policies (reduction, replacement, and refinement of the animal experimentation) have motivated a change of paradigm towards a more mechanistic view. Many international initiatives are promoting this change. Currently, the relevant toxicological data are obtained from a wide scope of studies, including in vitro assays, animal experiments, and clinical trials, and post-marketing observational studies. It has to be pointed out that most of these data are generated in a regulatory context, following guidelines for obtaining marketing approval. These data constitute an extremely valuable resource that should be made available to the scientific community. Efforts are being made for the systematic collection and storage of these data, as well as their standardization, which enables their integration and joint analysis.

## Data from in vitro assays
 
### Description
In vitro cell culture technologies are commonly used in toxicology. They provide an alternative to animal testing and allow to assess the response of the cells to a toxicant exposure. They also provide unique access to biochemical, and morphological changes that can not be observed in vivo.

Immortalized cell lines are cells that have been manipulated to proliferate indefinitely and can thus be cultured for long periods of time. In most cases, these cell lines
have been isolated from tumorigenic tissues or have been artificially manipulated to proliferate indefinitely. The disadvantage of using cell lines is that as a consequence of the manipulation, they may undergo alteration in their genomic content, and their gene expression profiles, so they do not resemble “normal” cells.

Primary cell cultures are derived from healthy human or animal tissues. Primary cells have normal diploid chromosomes and maintain much of the physiological properties of the tissue from which they are derived, so they are a better experimental model. Nevertheless, these cells have limited life spans: they experience senescence processes and have low potential for self-renewal and differentiation.

Although two-dimensional cell cultures are very popular, it has been shown that they do not represent the in vivo situation, as they are still far from the tissue organization and the cellular connections seen in an organism. Recent advances in three-dimensional cell culture technologies have allowed the widespread use of organoids. An organoid is a three-dimensional multicellular in vitro tissue structure, usually generated from stem cells or progenitor cells. They recreate the architecture and physiology of human organs in more detail than cell culture systems. Organoids can propagate for a long time with low predisposition to genomic alterations. Organoids have been used for in vitro modelling of drug adverse effects, specifically in organs commonly susceptible to drug-induced toxicities (i.e. gastrointestinal tract, liver, kidney). One of the disadvantages of the use of organoids to study the response to xenobiotics is that they cannot be used to study inflammatory responses because they lack immune cells.

In vitro tests in toxicology typically consist of exposing in vitro cell cultures to growing concentrations of the substance under study and recording changes using a wide variety of techniques, from high-content imaging to cell death.

Among the diverse sources of toxicological in vitro data it is worth mentioning the results of the Toxicology in the 21st Century program, or [Tox21](https://ntp.niehs.nih.gov/whatwestudy/tox21/index.html). This project is a collaboration between several federal agencies in the USA, started in 2004 to develop new ways to rapidly test whether substances adversely affect human health. The results of this project (data and tools) are publicly available.

Gene expression changes that occur in biological systems in response to exposure to xenobiotics may represent mechanistically relevant cellular events contributing to the onset and progression of xenobiotic-induced adverse health outcomes. Transcriptomics data can be used to identify changes to genetic expression profiles that occur in response to drug treatment. It involves the use of microarrays or, more recently, RNA sequencing (RNA-seq), which allows for the characterization of a biological system’s transcriptome in response to stimuli. Understanding how the transcriptome of an organ changes with exposure to a drug can provide predictive and mechanistic insight into the mode of action of a drug, as well as the molecular clues linked to possible toxicity. 

### Considerations

Results of in vitro assays are typically collected as dose-response curves. These results should be processed to obtain indexes indicating the concentration at which relevant effects are observed like LC50, IC50, benchmark concentration (BMC). This procedure can involve non-linear curve fitting and outlier removal. It is advisable to report the details of the data processing in order to obtain reproducible results.

### Solutions

- [ToxCast](https://cran.r-project.org/web/packages/tcpl/index.html) has published an R-package with the tools used to process the high throughput chemical screening data.
- Benchmark concentrations (and doses) can be computed with free software as [PROAST](https://www.rivm.nl/en/proast) and [BMDS](https://www.epa.gov/bmds).
- For experiments where gene expression has been measured in response to a toxicant, R packages such as [DESEq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html) for RNA-Seq data, and [limma](https://bioconductor.org/packages/release/bioc/html/limma.html) for microarray data are used to find genes that are differentially expressed.
- In silico prediction models can be developed starting from a series of compounds annotated with the results on in vitro methods. The quality of the predictions provided by these methods are often comparable with those obtained by experimental methods, particularly when the models are used within their applicability domain. [Flame](https://github.com/phi-grib/flame) is an open-source modelling framework developed specifically for this purpose.

## Data from animal assays

### Description

Assays are expensive. Most animal data come from compiling normative studies which are compulsory for obtaining the approval of diverse regulatory agencies prior to commercialization. The choice of the species and strains was determined by their representativity for the studied endpoints, and often defined in this legislation and comprises from invertebrate (e.g., daphnia is commonly used to study aquatic toxicity) and fish (e.g., zebrafish), to rodents (mice, rats, rabbits, guinea pigs) and mammals (dogs, primates).

In spite of their inconveniences (high costs, time consumption, requirements of significant amounts of the substance being tested, limited translatability of the observed results), in many cases, there is no suitable replacement for in vivo tests. The replacement of in vivo data for alternative approaches (often called NAM, New Approach methodologies) is an active research field.

Two important toxicogenomics resources containing animal data are [TG-GATEs](https://pubmed.ncbi.nlm.nih.gov/25313160/), and [DrugMatrix](https://pubmed.ncbi.nlm.nih.gov/25058030/). These resources contain gene expression data in several rat tissues for a large number of compounds, in several doses and exposure times. They also include histopathology annotations and chemistry measurements. Nevertheless, not all doses and times were assessed for every compound, and the data is not modeled using standards, for example, drugs are annotated using common names, and histopathology annotations are not coded in a controlled vocabulary. 

### Considerations

Data generated in normative studies were obtained under Good Laboratory Practices (GLP) conditions, and therefore the quality of the data is high, however, these studies were oriented to characterize a single compound, and not to carry out comparative analyses. Also, the doses used in the studies were designed to detect adversity and could be no representative of the exposure reached by consumers or patients of the marketed substances.

The representativity of animal data to predict human toxicity is questionable and a precautionary approach or the use of extrapolation factors is recommended.

### Solutions

- Use information about genes, and variants associated with human adverse effects, from platforms such as [DisGeNET](https://www.disgenet.org/), [CTD](http://ctdbase.org/), and [PharmGKB](https://www.pharmgkb.org/).
- Histopathology data requires the use of a controlled vocabulary like [CDISC/SEND](https://evs.nci.nih.gov/ftp1/CDISC/SEND/SEND%20Terminology.html).
- The extension and curation of ontologies like CDISC/SEND to specific domains is facilitated by tools like [ONTOBROWSER](https://opensource.nibr.com/projects/ontobrowser/).
- In order to reduce the number of animals used in toxicological studies, it has been suggested to replace control groups with historically collected data from studies carried out in comparable conditions (so-called Virtual Control Groups). VCGs are being developed by [eTRANSAFE project](https://etransafe.eu/virtual-control-groups-one-step-forward-into-the-future-of-animal-testing-in-toxicology).

## Data from human assays

### Description

Human response to toxic agents is generally excluded from toxicity assays as it entails major ethical issues. Although relevant information on potential adverse effects is available from animal and in vitro assays, human data is crucial for accurate calibration of toxicity models based on these studies. Traditionally, it was frequent that exposure to an unknown or unexpected toxic agent was eventually identified as the trigger factor of a health problem for which an evident reason did not apparently exist. Thereby, unintentional human exposure to toxic agents yielded toxicological data. Two main types of sources exist in this regard (i) Individual or group case reports are a fundamental source of information when no previously reported human toxicity information exists.  They include exhaustive medical information on a single patient or a set of patients with similar symptomatology which is gathered from health care facilities to identify etiology. (ii) Epidemiologic studies are focused on the possible association between the exposure to a substance and the potential adverse reactions observed in a given human population. ESs are classified as occupational (individuals are exposed in the workplace), or environmental (individuals are exposed through daily living).

In the pharmaceutical context though, intentional human exposure to drug candidates is a necessary step in the development of medications. During the clinical trial stage, human exposure to substances is required to characterize efficacy and safety. This process consists of several phases which are exhaustively controlled and subjected to strict regulations and ethical review. Adverse-event monitoring and reporting is a key issue in the assessment of the risk-benefit balance associated with the medication which is established from the clinical trials data. After the medication is released to the market it is subjected to an exhaustive pharmacovigilance process focused on the identification of safety concerns. Serious and non-serious adverse effects reporting from several sources are collected during a period and medication risk-benefit balance is re-evaluated.

### Considerations

Data from human assays are highly heterogeneous and integration with in vitro and animal data is a challenging task. There is a broad range of resources containing human data publicly available, but sometimes data access is limited. The nature of toxicological data has evolved in recent times and available resources and repositories comprise a variety of different types of data. On one hand, many data sources are nicely structured but, on the other hand, some others provide detailed information in an unstructured format. Data should be harmonized before integration. Disparate data sources are organized differently and also use different terminologies:
- Resources providing access to occupational epidemiologic studies report health risks by using condition-centred vocabularies like (ICD9-CM and ICD10-CM) or just uncoded terms whereas databases reporting possible links between observed adverse reactions and medications are usually expressed according to the MedDRA ontology.
- Different chemical identifiers are used depending on the toxic agent.
- Similarly, medication identifiers are not always consistent among different sources. This is a challenging issue as many medicinal products have different denominations and available commercial presentations depending on the country/region where the product is commercialized.
- Usually, structured resources present metadata explaining how the data is organized, thus enabling an easy data transformation process. Conversely, non-structured resources are not easy to harmonize as data organization is not consistent among the available documents.

Databases containing clinical toxicological data of drugs can contain the results of clinical studies ([clinicaltrials.gov](https://clinicaltrials.gov/)), frequent adversities (Medline), or collect pharmacovigilance data ([FAERS](https://www.fda.gov/drugs/surveillance/questions-and-answers-fdas-adverse-event-reporting-system-faers)) depending on the data being incorporated, the interpretation is different. For example, in the case of spontaneous reporting systems, the frequency with which an adverse event is reported should be considered relative to the time the compound has been in the market and the frequency of these adverse events in the population treated.

### Solutions

Harmonization of terminologies can be achieved by using different resources:

- The [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html) provides mappings between different medical vocabularies. It includes common ontologies within the condition/diagnosis domain like SNOMED, ICD9CM, ICD10CM, and also the MedDRA ontology.
- The [OHDSI](https://www.ohdsi.org/analytic-tools/athena-standardized-vocabularies/) initiative for health data harmonization is an alternative solution for the mapping of vocabularies needed for the harmonization of different resources. This initiative maintains the ATHENA set of vocabularies which is in constant evolution and covers relevant domains in the realm of health care. The OHDSI community is paying special attention to the mappings between medication identifiers coming from national regulatory agencies of the countries of provenance of the institutions involved in the initiative, and the RxNorm identifier which is the standard vocabulary used by OHDSI.

To import unstructured data sources into structured schemas is a really challenging task as it involves the application of natural language processing technologies. The development of these tools in the field of toxicology is still at the embrionary stage but several initiatives exist:

- The [LimTox](http://limtox.bioinfo.cnio.es/) system is a text mining approach devoted to the extraction of associations between chemical agents and hepatotoxicity.
- The [AOP4EUpest](http://www.biomedicale.parisdescartes.fr/aop4EUpest/home.php) webserver is a resource for the identification of annotated pesticides-biological events involved in Adverse Outcome Pathways (AOPs) via text mining approaches.

## Ecotoxicology data

### Description

Substances can also be characterized according to their potential to affect the environment. This data is collected by national and international regulatory agencies (e.g., ECHA in EU and, EPA in the USA) aiming to control the production, distribution, and use of potentially hazardous substances. Data collection is largely guided by legislation, which defines the test that should be carried out and the data that should be collected. 

### Considerations

When considering the effect of a substance on the environment, in addition to its hazard characterization, it is important to consider its environmental fate in terrestrial and aqueous environments, and its properties with respect to degradation by diverse routes (chemical, biodegradation, photodegradation).

### Solutions

- The [ECOTOXicology Knowledgebase (ECOTOX)](https://cfpub.epa.gov/ecotox/) is a comprehensive, publicly available Knowledgebase providing single chemical environmental toxicity data on aquatic life, terrestrial plants, and wildlife.
- The [CompTox Chemicals Dashboard](https://comptox.epa.gov/dashboard) provides toxicological information for over 800.000 chemical compounds, including experimental and predicted fate information.
